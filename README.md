# Sentiment Analysics

This repository contains a Jupyter notebook and helper script to perform sentiment analysis on Amazon product reviews (source CSV: `Reviews.csv`). The notebook provides both rule-based (VADER) and transformer-based (DistilBERT SST-2) sentiment scoring, basic EDA, and exports results.

Files in this folder (not exhaustive):

- `Ai-sentiments.ipynb` — Notebook with a full sentiment analysis pipeline (sampling for large files, preprocessing, VADER, Hugging Face transformer, EDA, and CSV export).
- `validate_vader.py` — Small script to quickly validate VADER scores on a 50-row sample of `Reviews.csv`.
- `Reviews.csv` — Original reviews CSV (large; not tracked by Git by default).
- `processed_reviews.csv` — Processed output CSV (large; not tracked by Git by default).
- `*.png` — Visualizations generated by the notebook (word clouds, distributions).

Screenshots / images included in this repo:

- `positive_reviews_word_cloud.png` — Word cloud for positive reviews.
- `negative_reviews_word_cloud.png` — Word cloud for negative reviews.
- `top_phrases_in_negative_reviews.png` — Top negative phrases.
- `sentiment_distribution.png` — Distribution of sentiments.

Large files note
----------------
The repository contains very large data files in the working directory. GitHub rejects files larger than 100 MB in normal pushes. The following files in this folder are large (rounded):

* processed_reviews.csv — ~456 MB
* database.sqlite — ~373 MB
* Reviews.csv — ~301 MB

Because of GitHub's file size limits, these CSV/DB files are not suitable for a regular git push. You have a few options:

1. Use Git LFS (recommended for keeping large files with the repo):

   - Install Git LFS on your machine: https://git-lfs.github.com/
   - Run the following in this repository root:

```powershell
git lfs install
git lfs track "*.csv"
git add .gitattributes
git add processed_reviews.csv Reviews.csv
git commit -m "Add large CSVs via Git LFS"
git push origin main
```

   Note: GitHub still enforces repository bandwidth/storage quotas for LFS; if your repo is public and you exceed quota you may need to purchase a plan or use alternative hosting.

2. Upload large files to a GitHub Release or a cloud storage bucket (Google Drive, AWS S3) and include a link in the README. This avoids git history bloat and is simple.

3. Split the CSV into smaller chunks (<100 MB each) and commit the chunks (less ideal).

How to run the notebook (quick)
------------------------------
1. Create and activate a Python environment (recommended):

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt  # or pip install pandas numpy matplotlib seaborn tqdm nltk scikit-learn transformers sentencepiece torch vaderSentiment
```

2. Open `Ai-sentiments.ipynb` in Jupyter or VS Code and run the cells. The notebook uses sampling by default (`PROCESS_FULL = False`) to avoid loading the entire CSV into memory. Set `PROCESS_FULL = True` to process everything (requires enough RAM and time).

3. The notebook saves results to `reviews_sentiment_results.csv` in the working directory.

If you'd like, I can:

- Add a `requirements.txt` and `README` improvements (including exact commands for your environment).
- Configure Git LFS in the repo and attempt to push the CSV files (I can try that next; if `git-lfs` isn't available on this machine the push will fail and I'll show the exact error and steps to fix it).
- Upload large files to a GitHub Release instead and add download links to this README.

Contact
-------
If you want me to proceed with pushing the large CSVs (via Git LFS) or uploading them to a release, tell me which option you prefer and I'll attempt it and report exact results.
